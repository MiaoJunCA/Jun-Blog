<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jun&#39;s Note Blog</title>
  
  <link href="/Jun-blog/atom.xml" rel="self"/>
  
  <link href="http://james97.github.io/"/>
  <updated>2016-04-26T18:44:38.000Z</updated>
  <id>http://james97.github.io/</id>
  
  <author>
    <name>Jun (James) Miao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Solve a machine learning problem step by step</title>
    <link href="http://james97.github.io/2016/04/26/Solve-a-machine-learning-problem-step-by-step/"/>
    <id>http://james97.github.io/2016/04/26/Solve-a-machine-learning-problem-step-by-step/</id>
    <published>2016-04-26T18:32:11.000Z</published>
    <updated>2016-04-26T18:44:38.000Z</updated>
    
    <content type="html">&lt;p&gt;####Main frame for data science problems&lt;/p&gt;
&lt;p&gt;My main problem is lack of experiences in dealing such problems, e.g. Cases on Kaggle. To be honest, with the help of open source packages, it is easy to get a nice result. Fetching the python code from a top-k author and modify it is an easy way. However, in order to find a data scientist job, it is better to learn how to solve these problems from the very beginning.&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Basically, we can divide the process into several steps: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Collect data. On Kaggle, data are ready in .csv files, so it is easy to read them through Pandas. For other scenarios, we need to collect data by ourselves and store them. The techniques or tools we can use are Nutch, Scrapy, Mysql, Solr/Lucene. &lt;/li&gt;
&lt;li&gt;&lt;p&gt;Load Data. Pandas has a series of readers to read data in different formats, which is very nice. Check the list below for what you need: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;read_csv &lt;/li&gt;
&lt;li&gt;read_excel &lt;/li&gt;
&lt;li&gt;read_hdf &lt;/li&gt;
&lt;li&gt;read_sql &lt;/li&gt;
&lt;li&gt;read_json &lt;/li&gt;
&lt;li&gt;read_msgpack (experimental) &lt;/li&gt;
&lt;li&gt;read_html &lt;/li&gt;
&lt;li&gt;read_gbq (experimental) &lt;/li&gt;
&lt;li&gt;read_stata - read_sas &lt;/li&gt;
&lt;li&gt;read_clipboard &lt;/li&gt;
&lt;li&gt;read_pickle &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data conversion. Usually, we use float vectors to represent each item and for the learning process, but the world is not so nice to provide such data only. So we may have free texts and categories. Luckily, we have Pandas and Sklearn to deal with them. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Texts. Vectorizer. For exmaple, a commonly used Vectorizer from sklearn package is TfidfVectorizer: &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;new_docs = [&lt;span class=&quot;string&quot;&gt;&#39;He watches basketball and baseball&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Julie likes to play basketball&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Jane loves to play baseball&#39;&lt;/span&gt;] &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;new_term_freq_matrix = tfidf_vectorizer.transform(new_docs) &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;print&lt;/span&gt; tfidf_vectorizer.vocabulary_ &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;print&lt;/span&gt; new_term_freq_matrix.todense()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#Terms are transformed to features, &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;the value number &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; the feature index of&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;this term &amp;#123;&lt;span class=&quot;string&quot;&gt;u&#39;me&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;basketball&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;julie&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;baseball&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;u&#39;likes&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;loves&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;jane&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;linda&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;more&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;u&#39;than&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;he&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;&amp;#125; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#tfidf vector for each document &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[[ &lt;span class=&quot;number&quot;&gt;0.57735027&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.57735027&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.57735027&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; ] [ &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.68091856&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.51785612&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.51785612&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; ] [ &lt;span class=&quot;number&quot;&gt;0.62276601&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.62276601&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.4736296&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt; ]]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To convert categories into integers, we can use Pandas with a map function: &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#Data are from the Titanic competition on Kaggle.com&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df[&lt;span class=&quot;string&quot;&gt;&#39;Gender&#39;&lt;/span&gt;] = &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;#Genders are converted to integers by the map function with a dictionary &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df[&lt;span class=&quot;string&quot;&gt;&#39;Gender&#39;&lt;/span&gt;] = df[&lt;span class=&quot;string&quot;&gt;&#39;Sex&#39;&lt;/span&gt;].map( &amp;#123;&lt;span class=&quot;string&quot;&gt;&#39;female&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;male&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&amp;#125; ).astype(int)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Data munging and data clean. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we want to remove a feature, just use Pandas’s drop() function. Droping items with NA features? Easy &lt;code&gt;pd.dropna()&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;Simply filling NA features. &lt;code&gt;pd.fillna()&lt;/code&gt;. Of course, there are lots of parameters we can use for filling. &lt;/li&gt;
&lt;li&gt;Complex filling. Sometimes, we want to fill the missing values more reasonably. For example, use the combination of other different features to determine the lost feature values. &lt;/li&gt;
&lt;li&gt;Feature creation. If we think we can make new features based on the current ones, it sometimes makes sense. For example, in the Titanic competition, we have the numbers of siblings and children. We can combine them to be a new features “familySize”. If we have the length and width of a rectangle, we can create a feature named “Area”. Actually, according to the &lt;a href=&quot;http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;paper&lt;/a&gt; about how to select features, features which have high variance with each other can also be complements. So don’t worry about the information overlap among features. &lt;/li&gt;
&lt;li&gt;Normalization. It is necessary to normalize features into the same scale like 0 to 1. No matter we plan to use linear models or not, this can help get a more reasonable result. &lt;/li&gt;
&lt;li&gt;Dimension deduction. PCA or LDA can reduce less important features and improve the learning speed. However, they have their own limit for particular scenarios. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Training. Select appropriate models for training. If you don’t know the best choice, try linear models or GBDT/RandomForest. Usually, they are not bad. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Testing. No matter cross-validation or others. Don’t use the same data for both training and testing. &lt;/li&gt;
&lt;li&gt;Use it. In Kaggle cases, use your trained model on the test data and submit it.&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;####Main frame for data science problems&lt;/p&gt;
&lt;p&gt;My main problem is lack of experiences in dealing such problems, e.g. Cases on Kaggle. To be honest, with the help of open source packages, it is easy to get a nice result. Fetching the python code from a top-k author and modify it is an easy way. However, in order to find a data scientist job, it is better to learn how to solve these problems from the very beginning.&lt;br&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://james97.github.io/tags/machine-learning/"/>
    
      <category term="data mining" scheme="http://james97.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Concepts (2)</title>
    <link href="http://james97.github.io/2016/04/20/Machine-Learning-Concepts-2/"/>
    <id>http://james97.github.io/2016/04/20/Machine-Learning-Concepts-2/</id>
    <published>2016-04-20T13:51:09.000Z</published>
    <updated>2016-04-20T17:27:25.000Z</updated>
    
    <content type="html">&lt;p&gt;Continue to the previous post.&lt;/p&gt;
&lt;h4 id=&quot;Bias-vs-Variance&quot;&gt;&lt;a href=&quot;#Bias-vs-Variance&quot; class=&quot;headerlink&quot; title=&quot;Bias vs Variance&quot;&gt;&lt;/a&gt;Bias vs Variance&lt;/h4&gt;&lt;p&gt;These two concepts have confused me for a long time, until I read something in &lt;strong&gt;Pattern Recognition and Machine Learning&lt;/strong&gt;. Generally, bias is the difference between the real model $G(x)$ and the model we learn $H(x)$; variance is the difference between $H(x)$ and $E(H(x))$. Here is a good picture to explain them:&lt;br&gt;&lt;img src=&quot;http://blog.fliptop.com/wp-content/uploads/2015/01/Bias_Variance.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The red spot is our target, or we can say $G(x)$. High bias means the learned model $H(x)$ is quite different from the real model, so we will get bad performance with this model. A high variance means the expectation of $H(x)$ is close to $E(G(x))$, but the result $H(x_i)$ for each sample $x_i$ is not close to $G(x_i)$. Ideally, we want both of them to be low. However, that is a kind of trade-off. The decrease of one will cause an increase of the other one. High variance learning methods may be able to represent the training set very well. But the world is not perfect, it will try to match everything, including the noise. So the model we get will be very complex and may fail for more new data. On the contrary, high-bias models are usually very simple, but fail to capture the important regularities. Models with low bias are usually very complex and obtain low training error. This complexity makes it hard to be generalized for other new samples, which gives high variance. &lt;/p&gt;
&lt;h4 id=&quot;Overfit-vs-Underfit&quot;&gt;&lt;a href=&quot;#Overfit-vs-Underfit&quot; class=&quot;headerlink&quot; title=&quot;Overfit vs Underfit&quot;&gt;&lt;/a&gt;Overfit vs Underfit&lt;/h4&gt;&lt;p&gt;These two concepts are connect with the above two. High variance leads to overfit, and high bias causes underfit. The following figure can describe them well.&lt;br&gt;&lt;img src=&quot;http://blog.fliptop.com/wp-content/uploads/2015/03/highvariance-300x254.png&quot; alt=&quot;&quot;&gt;. When training error is smaller and smaller, the probability of overfit increases because the model will be very complex (As we say above, this will cause a high variance because the model is too accurate for the training data. It will match noisy samples as well). A high-bias model will get a high training error with a simple form. So it can’t fit the training set well enough, or we can say “underfit”.&lt;/p&gt;
&lt;p&gt;The Figures are from &lt;a href=&quot;http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Fliptop Blog&lt;/a&gt;. This post gives a better description about these concepts.&lt;/p&gt;
&lt;h4 id=&quot;Regression-VS-Classification&quot;&gt;&lt;a href=&quot;#Regression-VS-Classification&quot; class=&quot;headerlink&quot; title=&quot;Regression VS Classification&quot;&gt;&lt;/a&gt;Regression VS Classification&lt;/h4&gt;&lt;p&gt;Regression methods are used to fit data for continuous targets, e.g., predict stock market value. Classification methods are for discrete targets, e.g., predict whether tomorrow will be sunny or rainy. &lt;/p&gt;
&lt;h4 id=&quot;Reinforced-learning&quot;&gt;&lt;a href=&quot;#Reinforced-learning&quot; class=&quot;headerlink&quot; title=&quot;Reinforced learning&quot;&gt;&lt;/a&gt;Reinforced learning&lt;/h4&gt;&lt;p&gt;This is different from deep learning. It does not have multiple layers. Instead, it has a environment state set, an action set and a set of state transition rules. Unlike traditional machine learning methods, it cannot be trained in one time. A typical scenario is playing chess. Each decision is determined by previous one or more steps. So the learning process is step by step, too. In this case, we setup a reward function instead of an error function. If we think a step is good, then we will give the learning agent a positive reward. Otherwise, it will be given a punishment. The problem becomes finding a step path with the maximum reward. &lt;/p&gt;
&lt;p&gt;Reinforced learning has been widely used in auto helicopter, robots, industrial control… It is a elegant solution for complex Markov Decision Process(MDP). A brief tutorial can be found &lt;a href=&quot;http://www.cs.ubc.ca/~murphyk/Bayes/pomdp.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Continue to the previous post.&lt;/p&gt;
&lt;h4 id=&quot;Bias-vs-Variance&quot;&gt;&lt;a href=&quot;#Bias-vs-Variance&quot; class=&quot;headerlink&quot; title=&quot;Bias vs Variance&quot;&gt;&lt;/a&gt;Bias vs Variance&lt;/h4&gt;&lt;p&gt;These two concepts have confused me for a long time, until I read something in &lt;strong&gt;Pattern Recognition and Machine Learning&lt;/strong&gt;. Generally, bias is the difference between the real model $G(x)$ and the model we learn $H(x)$; variance is the difference between $H(x)$ and $E(H(x))$. Here is a good picture to explain them:&lt;br&gt;&lt;img src=&quot;http://blog.fliptop.com/wp-content/uploads/2015/01/Bias_Variance.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://james97.github.io/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="http://james97.github.io/tags/machine-learning/"/>
    
      <category term="data mining" scheme="http://james97.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Concepts(1)</title>
    <link href="http://james97.github.io/2016/04/18/Machine-Learning-Concepts/"/>
    <id>http://james97.github.io/2016/04/18/Machine-Learning-Concepts/</id>
    <published>2016-04-18T04:05:06.000Z</published>
    <updated>2016-04-18T14:59:21.000Z</updated>
    
    <content type="html">&lt;p&gt;There are lots of machine learning concepts which look complex and mysterious. Before learning the ML techniques, it is necessary to understand them clearly. &lt;/p&gt;
&lt;h4 id=&quot;Machine-learning-vs-Data-mining&quot;&gt;&lt;a href=&quot;#Machine-learning-vs-Data-mining&quot; class=&quot;headerlink&quot; title=&quot;Machine learning vs Data mining&quot;&gt;&lt;/a&gt;Machine learning vs Data mining&lt;/h4&gt;&lt;p&gt;These two are very confusing for me. Honestly, I don’t think they have essential differences. Both of them means using computer techniques and statistic models to learn latent patterns from data. From the &lt;a href=&quot;http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/cccf07.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;review&lt;/a&gt;, Prof. Zhou says that “Machine learning” came from the AI area. Its purpose is making machines learn like real persons. The concept “Data mining” first came from the database area. It is close to “Knowledge discovery”. It is used to discovery useful knowledge from big amount of data. So we can consider DM as the inter-discipline of ML and database. A more formal definition from &lt;a href=&quot;https://class.coursera.org/ntumlone-003/lecture/11&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;coursera&lt;/a&gt; by Prof.Hsuan-Tien Lin is &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning: use data to computer hypothesis g that approximate target f&lt;/li&gt;
&lt;li&gt;Data mining: use (huge) data to find property.&lt;/li&gt;
&lt;li&gt;Traditional ML techniques concern on small scale data and DM deals with big data. ML is suitable for pattern recognition and prediction, DM can discovery associations among features(e.g., beer and diaper). &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Supervised-learning-vs-Unsupervised-learning&quot;&gt;&lt;a href=&quot;#Supervised-learning-vs-Unsupervised-learning&quot; class=&quot;headerlink&quot; title=&quot;Supervised learning vs Unsupervised learning&quot;&gt;&lt;/a&gt;Supervised learning vs Unsupervised learning&lt;/h4&gt;&lt;p&gt;Quite clear. Supervised learning rely on labelled data for training. It has very clear outputs. For example, classification or regression are this kind of applications. On the contrary, unsupervised learning do not need labelled data, so we cannot know that exact results we will get. A typical example is clustering. We know the data can be categorized into several groups, but we don’t know the group amount. In that case, there is no best result in general. Of course, to apply unsupervised learning, researchers proposed some metrics to evaluate its performance. &lt;/p&gt;
&lt;h4 id=&quot;Deep-learning-vs-Shallow-learning&quot;&gt;&lt;a href=&quot;#Deep-learning-vs-Shallow-learning&quot; class=&quot;headerlink&quot; title=&quot;Deep learning vs Shallow learning&quot;&gt;&lt;/a&gt;Deep learning vs Shallow learning&lt;/h4&gt;&lt;p&gt;Deep learning is extremely hot recently. AlphaGo is based on the technique. Facebook’s face recognition is based on it. It seems to be the god for AI. Is that true? Why is it “deep”?&lt;/p&gt;
&lt;p&gt;Traditional machine learning methods are called “shallow learning” because they only have no more than 1 hidden layer. What does that mean? These methods will not construct very complex non-linear model. For example, logistic regression is used to find a formula like $\frac{1}{1 + e^{-\theta X}}$. $\theta X$ is actually a dot product of $\theta$ and $X$, or we can say this function is essentially linear. SVM, no matter what kernel it uses, update the same formula by different data to obtain a better performance. Generally, the formula is already determined. What we do is optimize the parameters.&lt;/p&gt;
&lt;p&gt;One may ask about why traditional neural network is shallow too while it can be multi-layer. Let me talk about my opinion about machine learning first. Linear models are the simplest models. They can used to fit data which are not complex. That is why all ML courses start from them. In real world, however, things are not so perfect. We can say linear models are not suitable for most cases. Then how can we model non-linear data? A straightforward way is build a linear model over multiple linear sub-models. For example as follows:&lt;br&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/450px-Colored_neural_network.svg.png&quot; alt=&quot;ANN&quot;&gt; &lt;/p&gt;
&lt;p&gt;Each node in the hidden layer is the result of a linear model whose features are the inputs. Each node in the output layer is the result of a linear model of the hidden layer nodes. If we have more layers, we can build a very complex model and ideally fit any curves. Yes, this is the idea of neural networks. &lt;/p&gt;
&lt;p&gt;Wait, since we can have a multi-layer NN, why is it shallow? The problem is, traditional NN is optimized by the classic Back propagation(BP) method. A very detailed introduction is &lt;a href=&quot;http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;. I will not introduce how it works here. Because BP optimizes parameters layer by layer, the residues used for mid-layer optimization will be very small if there are many layers. So practically, there will not be more than 2 layers. &lt;/p&gt;
&lt;p&gt;How about deep learning? The concept “deep learning” is actually based on NN, and it is a multi-layer NN. The main difference is Professor Hinton found a new way to train the model and avoid the problem of BP. It can therefore support more layers. Facebook builds a 9-layer NN to recognize people’s face and obtain a 97.25% precision. I will read more references about deep learning and write a post in the future.  &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;There are lots of machine learning concepts which look complex and mysterious. Before learning the ML techniques, it is necessary to understand them clearly. &lt;/p&gt;
&lt;h4 id=&quot;Machine-learning-vs-Data-mining&quot;&gt;&lt;a href=&quot;#Machine-learning-vs-Data-mining&quot; class=&quot;headerlink&quot; title=&quot;Machine learning vs Data mining&quot;&gt;&lt;/a&gt;Machine learning vs Data mining&lt;/h4&gt;&lt;p&gt;These two are very confusing for me. Honestly, I don’t think they have essential differences. Both of them means using computer techniques and statistic models to learn latent patterns from data. From the &lt;a href=&quot;http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/cccf07.pdf&quot;&gt;review&lt;/a&gt;, Prof. Zhou says that “Machine learning” came from the AI area. Its purpose is making machines learn like real persons. The concept “Data mining” first came from the database area. It is close to “Knowledge discovery”. It is used to discovery useful knowledge from big amount of data. So we can consider DM as the inter-discipline of ML and database. A more formal definition from &lt;a href=&quot;https://class.coursera.org/ntumlone-003/lecture/11&quot;&gt;coursera&lt;/a&gt; by Prof.Hsuan-Tien Lin is &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning: use data to computer hypothesis g that approximate target f&lt;/li&gt;
&lt;li&gt;Data mining: use (huge) data to find property.&lt;/li&gt;
&lt;li&gt;Traditional ML techniques concern on small scale data and DM deals with big data. ML is suitable for pattern recognition and prediction, DM can discovery associations among features(e.g., beer and diaper).
    
    </summary>
    
      <category term="machine learning" scheme="http://james97.github.io/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="http://james97.github.io/tags/machine-learning/"/>
    
      <category term="data mining" scheme="http://james97.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>Build a light blog on Github by Hexo</title>
    <link href="http://james97.github.io/2016/04/15/Build-a-light-blog-on-Github-by-Hexo/"/>
    <id>http://james97.github.io/2016/04/15/Build-a-light-blog-on-Github-by-Hexo/</id>
    <published>2016-04-15T14:55:52.000Z</published>
    <updated>2016-04-15T18:35:59.000Z</updated>
    
    <content type="html">&lt;p&gt;In order to have my own blog, I have tried a lot of ways. A popular one is buy a domain name and a space,  download wordpress, use a template with plenty of plugins. The problem is, I have to select themes and plugins to satisfied my needs, e.g., code highlight or Markdown support. Is that too hard? Not really. But that will definitely cost much time, especially on dealing with css problems and plugin conflicts. Since usually I want to write some about what I learned in my area, it is better to start the blogging as easy as possible.&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;Github gives me the chance. It is the first time I know it can support blogs in a handy way. There are lots of posts talking about how to build it, and the guidance from Github is good enough. Here I want to record how I use &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;hexo&lt;/a&gt; to get this done and what problems I have met.&lt;/p&gt;
&lt;p&gt;Hexo is a jekyll like tool for generating static website, which is quite suitable for Github pages. I have tried jekyll. It is good, but still not so convenient as hexo. I have to update everything as using a normal git repository with commit/push operations. On the contrary, hexo can get these done with one single command.&lt;/p&gt;
&lt;p&gt;Installing hexo is very easy. First of all, make sure node.js is installed. On mac, just use &lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;brew install node&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Of course, you have to install git as well. Then what’s next? Another two commands are enough&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;npm install hexo-cli -g&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;npm install hexo-deployer-git --save&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Then everything is done.&lt;/p&gt;
&lt;p&gt;Now create a new folder for your blog site. For example, ~/hexo as I did. Following the next steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;enter the hexo folder and run &lt;code&gt;hexo init&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;config _config.yml for your own needs&lt;/li&gt;
&lt;li&gt;execute `hexo generate’ to generate a template site&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;hexo server&lt;/code&gt; and then open &lt;code&gt;localhost:4000&lt;/code&gt; in your browser&lt;/li&gt;
&lt;li&gt;if you can see the website, then everything is fine&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;hexo new [layout] &amp;quot;title&amp;quot;&lt;/code&gt; to create a new post. For example, &lt;code&gt;hexo new post &amp;quot;hello world&amp;quot;&lt;/code&gt;. Then you will have a new &lt;strong&gt;md&lt;/strong&gt; file in source/_posts folder.&lt;/li&gt;
&lt;li&gt;Use a editor to write whatever you like and save it. Here I highly recommend a markdown editor &lt;a href=&quot;http://macdown.uranusjr.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Macdown&lt;/a&gt;. It is open-source and has enough functions for most blogger. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;hexo deploy&lt;/code&gt; to upload your post to your github blog.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are several things we need to concern. The most important thing is the _config.yml. This file controls all the settings of your blog. It is easy to setup title/description or other site attributes. For the URL part, take care. Initially, I think I don’t have a personal web space and domain name, so I needn’t change this. The result is, no matter how I deploy it, my blog on github does not execute the css style file. Also, every link on the site will be directed to &lt;code&gt;404&lt;/code&gt; page. So even you just use your github url, define it in the _config.yml file as follows please&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;url: http://james97.github.io/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;root: /Jun-blog/ #project name&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Another point is about the deploy settings. Here is mine&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;deploy: &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    type: git&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    repository: git@github.com:james97/Jun-blog.git&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    branch: gh-pages&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Some old posts set type to &lt;code&gt;github&lt;/code&gt;. It is not correct now. The branch issue is also important. The real blog shown to readers is always in your &lt;code&gt;gh-pages&lt;/code&gt; branch. When you create a repository for the blog, you will have a &lt;code&gt;master&lt;/code&gt; branch at first. Then go to settings and click &lt;code&gt;Launch automatic page generator&lt;/code&gt;, you will have a &lt;code&gt;gh-pages&lt;/code&gt; branch automatically. Every time you want to update your blog, modify this branch. The &lt;code&gt;master&lt;/code&gt; branch is used for your real projects, not the blogs.&lt;/p&gt;
&lt;p&gt;If you want to use some fancy themes, you can find them &lt;a href=&quot;https://hexo.io/themes/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;. All of them are on Github.com. Clone them into the &lt;code&gt;/themes/&lt;/code&gt; folder and update them as normal git projects. Use &lt;code&gt;hexo generate&lt;/code&gt; to update the theme. After the update, check how it works by run &lt;code&gt;hexo server&lt;/code&gt; locally. A common problem is &lt;code&gt;cannot find index.htm&lt;/code&gt;. The reason is hexo cannot find the according theme set in _config.yml. Remember to keep a blank between &lt;code&gt;theme:&lt;/code&gt; and the theme name. Another problem is when your posts/pages are not compatible with the theme. In this case, there will a lot of error when executing &lt;code&gt;hexo g&lt;/code&gt;. Currently I am using the &lt;a href=&quot;https://github.com/litten/hexo-theme-yilia&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;yilia&lt;/a&gt; theme, which is pretty cool. &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;In order to have my own blog, I have tried a lot of ways. A popular one is buy a domain name and a space,  download wordpress, use a template with plenty of plugins. The problem is, I have to select themes and plugins to satisfied my needs, e.g., code highlight or Markdown support. Is that too hard? Not really. But that will definitely cost much time, especially on dealing with css problems and plugin conflicts. Since usually I want to write some about what I learned in my area, it is better to start the blogging as easy as possible.&lt;br&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://james97.github.io/categories/Programming/"/>
    
    
      <category term="hexo" scheme="http://james97.github.io/tags/hexo/"/>
    
      <category term="github" scheme="http://james97.github.io/tags/github/"/>
    
      <category term="blog" scheme="http://james97.github.io/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://james97.github.io/2016/04/13/hello-world/"/>
    <id>http://james97.github.io/2016/04/13/hello-world/</id>
    <published>2016-04-14T02:44:14.000Z</published>
    <updated>2016-04-15T16:00:40.000Z</updated>
    
    <content type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/writing.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Writing&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Run-server&quot;&gt;&lt;a href=&quot;#Run-server&quot; class=&quot;headerlink&quot; title=&quot;Run server&quot;&gt;&lt;/a&gt;Run server&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo server&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/server.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Server&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Generate-static-files&quot;&gt;&lt;a href=&quot;#Generate-static-files&quot; class=&quot;headerlink&quot; title=&quot;Generate static files&quot;&gt;&lt;/a&gt;Generate static files&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo generate&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/generating.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Generating&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Deploy-to-remote-sites&quot;&gt;&lt;a href=&quot;#Deploy-to-remote-sites&quot; class=&quot;headerlink&quot; title=&quot;Deploy to remote sites&quot;&gt;&lt;/a&gt;Deploy to remote sites&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo deploy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/deployment.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deployment&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
