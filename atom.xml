<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jun&#39;s Note Blog</title>
  
  <link href="/Jun-Blog/atom.xml" rel="self"/>
  
  <link href="https://miaojunca.github.io/"/>
  <updated>2017-02-20T21:23:42.000Z</updated>
  <id>https://miaojunca.github.io/</id>
  
  <author>
    <name>Jun (James) Miao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>how to use lucene (2)</title>
    <link href="https://miaojunca.github.io/2017/02/20/how-to-use-lucene-2/"/>
    <id>https://miaojunca.github.io/2017/02/20/how-to-use-lucene-2/</id>
    <published>2017-02-20T20:37:15.000Z</published>
    <updated>2017-02-20T21:23:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>#####Search<br>After building an index, we need to implement search method for real usage. A quick search can be implemented in several lines:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MultiFieldQueryParser parser = <span class="keyword">new</span> MultiFieldQueryParser(searchFields, analyzer);</div><div class="line">Query query = parser.parse(<span class="string">"Ipad"</span>);</div><div class="line">TopDocs topDocs = indexsearcher.search(query, returnedDocNum);</div><div class="line">ScoreDoc[] docs = topDocs.scoreDocs;</div></pre></td></tr></table></figure>
<p>So, to search something, we need to set some variables first.<br><a id="more"></a></p>
<ol>
<li>A query parser. Query parser will convert a string into a Lucene-format query which includes a set of “clause”. Each clause will be the combination of a field and a tokenized query term. So here we have to set the field array we want to search with according analyzer.</li>
<li>Query. After we use the parser to analyze the raw query, we get a query.</li>
<li>An IndexReader. IndexReader objects can be created by new IndexReader(indexPath). For all statistics we want to use, we need to obtain from an index reader. More examples can be found at Lucene’ official Javadoc <a href="https://lucene.apache.org/core/5_1_0/core/org/apache/lucene/index/package-summary.html" target="_blank" rel="external">here</a>. The document can show you how to get the most useful information in an index. </li>
<li>The returned docs are in ScoreDoc format. The class includes the doc internal id and its according score.</li>
<li>A Similarity function! This is the core of a search engine. According to my experience, ClassicSimilarity is very good for common usage. More information is <a href="http://www.lucenetutorial.com/advanced-topics/scoring.html" target="_blank" rel="external">here</a>. Although Lucene has ben updated many times, the scoring formula is not changed. </li>
</ol>
<p>One thing I want to mention is that for different similarity function, the formula can be quite different! The most efficient way to know how a similarity function works is to print its explanation as follows</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Explanation e = searcher.explain(query, docId);</div><div class="line">System.out.println(e.toString())</div></pre></td></tr></table></figure>
<p>For example, if you use ClassicSimilarity, you will see the output</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">3.4333777 = sum of:</div><div class="line">  0.8260998 = sum of:</div><div class="line">    0.45502663 = weight(title:kindle in 50234) [ClassicSimilarity], result of:</div><div class="line">      0.45502663 = score(doc=50234,freq=1.0), product of:</div><div class="line">        0.332346 = queryWeight, product of:</div><div class="line">          6.258904 = idf(docFreq=1909, docCount=367236)</div><div class="line">          0.05309971 = queryNorm</div><div class="line">        1.3691353 = fieldWeight in 50234, product of:</div><div class="line">          1.0 = tf(freq=1.0), with freq of:</div><div class="line">            1.0 = termFreq=1.0</div><div class="line">          6.258904 = idf(docFreq=1909, docCount=367236)</div><div class="line">          0.21875 = fieldNorm(doc=50234)</div></pre></td></tr></table></figure>
<p>The hierarchy structure reveal the process of calculating the score of a clause given a document field. In the above example, we calculate <strong>fieldWeight</strong> 1.36911353 from the product of tf, idf and fieldNorm.</p>
<p>A trick to get the score of just one document given a query and a similarity function is using explanation, too. When we get the Explanation e, <strong>e.getValue()</strong> is what we need. Straightforward!</p>
<p>So, search in Lucene is totally field-based. If you want to search cross fields, the only way I know is create a new field including the contents of other several. </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;#####Search&lt;br&gt;After building an index, we need to implement search method for real usage. A quick search can be implemented in several lines:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;MultiFieldQueryParser parser = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; MultiFieldQueryParser(searchFields, analyzer);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Query query = parser.parse(&lt;span class=&quot;string&quot;&gt;&quot;Ipad&quot;&lt;/span&gt;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;TopDocs topDocs = indexsearcher.search(query, returnedDocNum);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;ScoreDoc[] docs = topDocs.scoreDocs;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;So, to search something, we need to set some variables first.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://miaojunca.github.io/categories/Programming/"/>
    
    
      <category term="Information Retrieval" scheme="https://miaojunca.github.io/tags/Information-Retrieval/"/>
    
      <category term="Lucene" scheme="https://miaojunca.github.io/tags/Lucene/"/>
    
  </entry>
  
  <entry>
    <title>how to use lucene (1)</title>
    <link href="https://miaojunca.github.io/2017/02/12/how-to-use-lucene-1-1/"/>
    <id>https://miaojunca.github.io/2017/02/12/how-to-use-lucene-1-1/</id>
    <published>2017-02-13T00:49:44.000Z</published>
    <updated>2017-02-13T00:55:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>Lucene is the core of ElasticSearch and Solr. If I want to customize a search engine and make improvements via new components, it is necessary to access Lucene directly. A reference can be found <a href="https://www.javacodegeeks.com/2015/09/building-a-search-index-with-lucene.html" target="_blank" rel="external">Here</a>. Some examples are also from this blog. The author owns the credit.</p>
<p>#####Build Index<br>Everything starts from building an index. The structure of Lucene processes haven’t been changed in the recent versions.</p>
<p>Step 1. Set an <strong>IndexWriter</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Directory index = FSDirectory.open(Paths.get(indexPath));</div><div class="line">IndexWriterConfig config = <span class="keyword">new</span> IndexWriterConfig().setSimilarity(<span class="keyword">this</span>.similarity)</div><div class="line">                .setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);</div><div class="line">IndexWriter indexWriter = <span class="keyword">new</span> IndexWriter(index, config);</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>A thing confused me is that we need to set a similarity for IndexWriter!! The only reason I guess is Similarity contains the way of length normalization. By default, it is set to BM25Similarity. </p>
<p>Step 2. Convert raw texts into Lucene Documents.<br>In Lucene, indices consists lots of documents, and each document consists a set of fields. So when searching in Lucene, search fields always need to be specified as a string array.</p>
<p>Quoted from Lucene Javadoc:<br>Expert: directly create a field for a document. Most users should use one of the sugar subclasses:</p>
<ul>
<li>TextField: Reader or String indexed for full-text search</li>
<li>StringField: String indexed verbatim as a single token</li>
<li>IntPoint: int indexed for exact/range queries.</li>
<li>LongPoint: long indexed for exact/range queries.</li>
<li>FloatPoint: float indexed for exact/range queries.</li>
<li>DoublePoint: double indexed for exact/range queries.</li>
<li>SortedDocValuesField: byte[] indexed column-wise for sorting/faceting</li>
<li>SortedSetDocValuesField: SortedSet<byte[]> indexed column-wise for sorting/faceting</byte[]></li>
<li>NumericDocValuesField: long indexed column-wise for sorting/faceting</li>
<li>SortedNumericDocValuesField: SortedSet<long> indexed column-wise for sorting/faceting</long></li>
<li>StoredField: Stored-only value for retrieving in summary results </li>
</ul>
<p>Of course, we want texts to be stored/indexed in different formats. For example, real contents should be analyzed into tokens and used for searches. Differently, we don’t want to change Id fields a little bit because they are unique. For numbers, we expect to have a range search over that. So seriously think about the structure of a document before index it. A lesson from me is that I store the content as a StringField. In this case, “cat” will not match documents which have “cat and dog” because “cat and dog” is considered as a single token. It is smart to check the source code of each Field class before use it.</p>
<p>Want to customize a field? No problem. </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">FieldType storedFieldType = <span class="keyword">new</span> FieldType();</div><div class="line">      storedFieldType.setStoreTermVectors(<span class="keyword">true</span>);</div><div class="line">      storedFieldType.setStored(<span class="keyword">true</span>);</div><div class="line">      storedFieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);</div><div class="line">  	doc.add(<span class="keyword">new</span> Field(fieldName, (String) sth, storedFieldType));</div></pre></td></tr></table></figure>
<p>I use the above field to store a kind of text with TermVectors so that I can know what terms are in this field in the future. By default, this option is <strong>FALSE</strong>. Field.Index.ANALYZED means the content will be analyzed and can be searched. Field.Store.Yes means the field can be shown for the final search results, but it cannot be searched…</p>
<p>Step 3. Set an analyzer for the indexwriter. An analyzer is for proceed a text into tokens(Or approximately, terms in the index). </p>
<blockquote>
<p>To make sure a search is correct, we have to use the same analyzer for indexing and searching, and Lucene itself will not ensure that!</p>
</blockquote>
<p>By default, Lucene use StandardAnalyzer. It will do stemming/stopwords removal.</p>
<p>Step 4. Wanna boost the weight of a Field? This can be done as follows. But it looks like the boost is on the document level which means only this document will benefit from the boost.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">doc.add(<span class="keyword">new</span> TextField(<span class="string">"author"</span>, author, Field.Store.YES));</div><div class="line"> </div><div class="line">      Field titleField = <span class="keyword">new</span> TextField(<span class="string">"title"</span>, title, Field.Store.YES);</div><div class="line">      doc.add(titleField);</div><div class="line">      titleField.setBoost(titleBoost);</div><div class="line"> </div><div class="line">      indexWriter.addDocument(doc);</div></pre></td></tr></table></figure>
<p>To apply a query-time boost is also available. Before building the real query, one can boost the weight of a query term in a particular field as:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">TermQuery query = <span class="keyword">new</span> TermQuery(<span class="keyword">new</span> Term(<span class="string">"title"</span>, <span class="string">"fox"</span>));</div><div class="line">         TermQuery query2 = <span class="keyword">new</span> TermQuery(<span class="keyword">new</span> Term(<span class="string">"author"</span>, <span class="string">"smith"</span>));</div><div class="line">         TermQuery query3 = <span class="keyword">new</span> TermQuery(<span class="keyword">new</span> Term(<span class="string">"author"</span>, <span class="string">"sam"</span>));</div><div class="line">         query2.setBoost((<span class="keyword">float</span>) <span class="number">2.0</span>);</div><div class="line"> </div><div class="line">         BooleanQuery booleanQuery = <span class="keyword">new</span> BooleanQuery();</div><div class="line">         booleanQuery.add(query, Occur.MUST);</div><div class="line">         booleanQuery.add(query2, Occur.SHOULD);</div><div class="line">         booleanQuery.add(query3, Occur.SHOULD);</div></pre></td></tr></table></figure>
<p>Now smith appearing in “author” field will get a higher score than normal.</p>
<p>Step 5. Add new documents to IndexWriter and write as normal file write operations. </p>
<p>Extra: If one wants to add a new document after an index has been built, that is also easy. Just create an indexreader as above, but using IndexReaderConfig to set it in “append” mode.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">IndexWriterConfig conf = <span class="keyword">new</span> IndexWriterConfig(analyzer);</div><div class="line">conf.setOpenMode(IndexWriterConfig.OpenMode.APPEND);</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Lucene is the core of ElasticSearch and Solr. If I want to customize a search engine and make improvements via new components, it is necessary to access Lucene directly. A reference can be found &lt;a href=&quot;https://www.javacodegeeks.com/2015/09/building-a-search-index-with-lucene.html&quot;&gt;Here&lt;/a&gt;. Some examples are also from this blog. The author owns the credit.&lt;/p&gt;
&lt;p&gt;#####Build Index&lt;br&gt;Everything starts from building an index. The structure of Lucene processes haven’t been changed in the recent versions.&lt;/p&gt;
&lt;p&gt;Step 1. Set an &lt;strong&gt;IndexWriter&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Directory index = FSDirectory.open(Paths.get(indexPath));&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;IndexWriterConfig config = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; IndexWriterConfig().setSimilarity(&lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;.similarity)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                .setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;IndexWriter indexWriter = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; IndexWriter(index, config);&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://miaojunca.github.io/categories/Programming/"/>
    
    
      <category term="Information Retrieval" scheme="https://miaojunca.github.io/tags/Information-Retrieval/"/>
    
      <category term="Lucene" scheme="https://miaojunca.github.io/tags/Lucene/"/>
    
  </entry>
  
  <entry>
    <title>how to use setuptools for python projects</title>
    <link href="https://miaojunca.github.io/2016/10/15/how-to-use-setuptools-for-python-projects/"/>
    <id>https://miaojunca.github.io/2016/10/15/how-to-use-setuptools-for-python-projects/</id>
    <published>2016-10-16T00:42:00.000Z</published>
    <updated>2016-10-16T18:49:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>When publishing a Python project, it is nice to have a powerful tool and package everything flexibly. Fortunately, we have <strong>setuptools</strong>.</p>
<p>To package a project, first you need to compose a setup.py. A sample is like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup, find_packages</div><div class="line">setup(</div><div class="line">    name=<span class="string">"HelloWorld"</span>,</div><div class="line">    version=<span class="string">"0.1"</span>,</div><div class="line">    packages=find_packages(),</div><div class="line">    scripts=[<span class="string">'say_hello.py'</span>],</div><div class="line"></div><div class="line">    <span class="comment"># Project uses reStructuredText, so ensure that the docutils get</span></div><div class="line">    <span class="comment"># installed or upgraded on the target machine</span></div><div class="line">    install_requires=[<span class="string">'docutils&gt;=0.3'</span>],</div><div class="line"></div><div class="line">    package_data=&#123;</div><div class="line">        <span class="comment"># If any package contains *.txt or *.rst files, include them:</span></div><div class="line">        <span class="string">''</span>: [<span class="string">'*.txt'</span>, <span class="string">'*.rst'</span>],</div><div class="line">        <span class="comment"># And include any *.msg files found in the 'hello' package, too:</span></div><div class="line">        <span class="string">'hello'</span>: [<span class="string">'*.msg'</span>],</div><div class="line">    &#125;,</div><div class="line"></div><div class="line">    <span class="comment"># metadata for upload to PyPI</span></div><div class="line">    author=<span class="string">"Me"</span>,</div><div class="line">    author_email=<span class="string">"me@example.com"</span>,</div><div class="line">    description=<span class="string">"This is an Example Package"</span>,</div><div class="line">    license=<span class="string">"PSF"</span>,</div><div class="line">    keywords=<span class="string">"hello world example examples"</span>,</div><div class="line">    url=<span class="string">"http://example.com/HelloWorld/"</span>,   <span class="comment"># project home page, if any</span></div><div class="line"></div><div class="line">    <span class="comment"># could also include long_description, download_url, classifiers, etc.</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>find_package() is a sweet function, which will search all the packages in your project. So now you don’t have to write a list of your packages. </p>
<p>scripts is the entrance of your project. </p>
<p>package_data is all the non-py files you need for your project. However, this is not the best way to include your data. A recommended way is using MANIFEST.in:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">include *.txt</div><div class="line">recursive-include examples *.txt *.py</div><div class="line">prune examples/sample?/build</div></pre></td></tr></table></figure>
<p>Easy to understand! Do remember put MANIFEST.in and setup.py on the top level of your project. Also, put <strong>inlude_package_data=True</strong> in your setup.py. </p>
<p>An interesting thing is that, when I finished my configuration and run <code>pip install .</code>, I failed to package all the non-py files to my library path. After I move <strong>inlude_package_data=True</strong> before calling find_packages(), it works. However, I can’t replicate this after that. Hmmm, if you meet the same problem, you can try this.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When publishing a Python project, it is nice to have a powerful tool and package everything flexibly. Fortunately, we have &lt;strong&gt;setupt
    
    </summary>
    
    
      <category term="programming" scheme="https://miaojunca.github.io/tags/programming/"/>
    
      <category term="python" scheme="https://miaojunca.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Understanding character encoding</title>
    <link href="https://miaojunca.github.io/2016/10/05/Understanding-character-encoding/"/>
    <id>https://miaojunca.github.io/2016/10/05/Understanding-character-encoding/</id>
    <published>2016-10-06T01:48:06.000Z</published>
    <updated>2016-10-15T23:30:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>Different charset encodings have confused me for a long time. When I use python or Java to deal with texts, when and how to encode/decode them are always questions for me. Now, it is time to understand the issue thoroughly!<br><a id="more"></a></p>
<h5 id="Why-so-many-charsets"><a href="#Why-so-many-charsets" class="headerlink" title="Why so many charsets?"></a>Why so many charsets?</h5><p>At the very beginning, there was only one code for english: Ascii. Use 1 byte to represent 256 chars. Actually, only half of the codes were used(under \x80). </p>
<p>However, there are so many languages in the world, and we need to encode most of them into computer system. Obviously, 1 byte was not enough. Then, how about 2 or 4 bytes? That was the idea! Different languages used different sizes of bytes for representations. If the first byte is below \x80, then it is an Ascii code. Otherwise, read the next byte and make it a ‘word’, skip the next byte and go on. Theoretically, 2bytes can represent 65536 chars for a particular language. But for some like Chinese, that is not enough. So some Chinese chars cannot be recognized correctly in computer systems.</p>
<p>MBCS(Multiple-byte Character set) is the set of this kind of encoding methods. </p>
<h5 id="How-about-use-one-charset-for-all"><a href="#How-about-use-one-charset-for-all" class="headerlink" title="How about use one charset for all?"></a>How about use one charset for all?</h5><p>That is an intuitive question. Just use more bytes and include all chars in the world! Of course, that the reason we have unicode. Now it can have more than 1M chars and every char has its unique code. E.g., U+0041 is ‘A’ in English, and U+4E25 is ‘严’ in Chinese. Perfect! Isn’t it?</p>
<h5 id="New-problem"><a href="#New-problem" class="headerlink" title="New problem"></a>New problem</h5><p>As I mentioned above, different languages have their own encoding method, like in MBCS. If we use unicode, how can we know that language we should recognize? U+4E25 need 15 bits, and some char need more. Also, how do we know a char is stored in unicode or ascii? One may say we can define the length of unicode to be 4 or 8 bytes, which should be enough for all languages. The generated problem is that we have to pad lots of 0s for English chars. That is a waste!</p>
<h5 id="Compromise-Utf-8-and-Utf-16"><a href="#Compromise-Utf-8-and-Utf-16" class="headerlink" title="Compromise: Utf-8 and Utf-16"></a>Compromise: Utf-8 and Utf-16</h5><p>Using a unified encoding/decoding system is necessary for the world. UTF-8 and UTF-16 were born for this. Both of them are the implementation of Unicode. UTF-8 is the most popular one, and it is also the default encoding method for most editors. It has two rules:</p>
<ol>
<li>For 1-byte chars, the first bit is always 0. The following 7 bits are Unicode code. So for English chars, they have the same utf-8 and ascii code. </li>
<li>For multi-byte, e.g., n chars, the first n bits of the first byte are always 1, and the <strong>n+1</strong>-th bit is 0. The first two bits of the rest bytes are set to 10, and the remain bits are the unicode of the char. </li>
</ol>
<p>UTF-16 uses 2 or 4 bytes while UTF-8 can be as short as 1 byte. Because of the byte limit, some chars cannot be shown in UTF-8 or UTF-16, but most regular chars are good in use. </p>
<p>Here is a small table for determining how many bytes UTF-8 will use for a char.</p>
<table>
<thead>
<tr>
<th>Unicode</th>
<th style="text-align:center">UTF-8 Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>0000 0000-0000 007F</td>
<td style="text-align:center">0xxxxxxx</td>
</tr>
<tr>
<td>0000 0080-0000 07FF</td>
<td style="text-align:center">110xxxxx 10xxxxxx</td>
</tr>
<tr>
<td>0000 0800-0000 FFFF</td>
<td style="text-align:center">1110xxxx 10xxxxxx 10xxxxxx</td>
</tr>
<tr>
<td>0001 0000-0010 FFFF</td>
<td style="text-align:center">11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</td>
</tr>
</tbody>
</table>
<h5 id="ANSI"><a href="#ANSI" class="headerlink" title="ANSI"></a>ANSI</h5><p>Sometimes we see ANSI for encoding. What is that? For English files, ANSI is ASCII; For Chinese files, it is GB2313 or Big5</p>
<h5 id="How-about-in-Python"><a href="#How-about-in-Python" class="headerlink" title="How about in Python"></a>How about in Python</h5><p>Yes. I met plenty of encoding errors in my Python programs and that made me crazy. This is also the reason of writing this blog. It is common to see </p>
<blockquote>
<p>“UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position 0-1: ordinal not in range(128)”</p>
</blockquote>
<p>and other similar ones. If you have special chars like Chinese in your source code, put</p>
<blockquote>
<p># -<em>- coding: utf-8 -</em>-<br>on the top of your file. </p>
</blockquote>
<p>If this happens because you are processing some non-english data, you need something else. In python 2.x, we have decode and encode methods for string. How to use them? Remember unicode? That is the common ‘language’ for all encoding systems. So </p>
<blockquote>
<p>decode() will decode the string into unicode. E.g., str1.decode(‘gb2312’) will decode the gb2312 string into unicode<br>encode(), obviously, will convert the string to a code other than unicode. str2.encode(‘gb2312’) will convert the unicode str2 into gb2312.</p>
</blockquote>
<p>So always figure out what is the current charset. Otherwise, you can’t successfully convert strings. Can use <strong>sys.getdefaultencoding()</strong> to get the current encoding system.</p>
<p>Btw, if you use ‘u’ in python files, e.g., s=u’中文’ means s is a unicode string. </p>
<p>Some IDE will show wrong chars in console. The reason is that it has its own encoding system for console output. If the encoding system is ascii, you will get error even if you define a Chinese string s in unicode. In this case, encoding the string before you print.  </p>
<h5 id="Python3-Something-new"><a href="#Python3-Something-new" class="headerlink" title="Python3? Something new"></a>Python3? Something new</h5><p>In python3, all strings are in unicode by default. A new data type <strong>bytes</strong> is introduced. It is actually used for ascii chars.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">b'byte literal'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">1</span>]</div><div class="line"><span class="number">121</span></div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>See? Every element in the string is a 0-127 ascii char. A bytes object can be converted into a unicode string by decode(). Again, because now every string is in unicode, strings only have encode() method now. </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Different charset encodings have confused me for a long time. When I use python or Java to deal with texts, when and how to encode/decode them are always questions for me. Now, it is time to understand the issue thoroughly!&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://miaojunca.github.io/categories/programming/"/>
    
    
      <category term="programming" scheme="https://miaojunca.github.io/tags/programming/"/>
    
      <category term="python" scheme="https://miaojunca.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>How to set python version in virtualenv</title>
    <link href="https://miaojunca.github.io/2016/10/05/How-to-set-python-version-in-virtualenv/"/>
    <id>https://miaojunca.github.io/2016/10/05/How-to-set-python-version-in-virtualenv/</id>
    <published>2016-10-06T00:11:43.000Z</published>
    <updated>2016-10-06T01:41:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I need to use Python 3.5 instead of Python 2.7, so I use virtualenv to build a particular environment for this project.</p>
<p>One way is set it when creating the virtual environment</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mkvirtualenv --python=/usr/bin/python3 nameOfEnvironment</div></pre></td></tr></table></figure>
<p>The second way is adding this line to the bashrc file:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> VIRTUALENV_PYTHON=/usr/bin/python3</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I need to use Python 3.5 instead of Python 2.7, so I use virtualenv to build a particular environment for this project.&lt;/p&gt;
&lt;p&gt;O
    
    </summary>
    
      <category term="programming" scheme="https://miaojunca.github.io/categories/programming/"/>
    
    
      <category term="programming" scheme="https://miaojunca.github.io/tags/programming/"/>
    
      <category term="python" scheme="https://miaojunca.github.io/tags/python/"/>
    
      <category term="virtualenv" scheme="https://miaojunca.github.io/tags/virtualenv/"/>
    
  </entry>
  
  <entry>
    <title>insert local image for hexo sites</title>
    <link href="https://miaojunca.github.io/2016/05/24/insert-local-image-for-hexo-sites/"/>
    <id>https://miaojunca.github.io/2016/05/24/insert-local-image-for-hexo-sites/</id>
    <published>2016-05-24T13:18:04.000Z</published>
    <updated>2016-05-24T13:53:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>It is very common to insert images into a markdown file. Usually, we can easily make this by “![]()” and put the URL of the pictures into (). How can we deal with local pictures? </p>
<p>I have to say, if editors like MacDown can be associated to Hexo better, that will be as awesome as <a href="https://marxi.co/" target="_blank" rel="external">Marxico</a>. The latter can automatically put your snapped figures into local ones, insert links into your md file and convert your md to a Evernote note. You don’t even know where the figures are really stored! </p>
<p>The good thing is we have a plugin named <a href="https://github.com/CodeFalling/hexo-asset-image" target="_blank" rel="external">hexo-asset-image</a>. Just enter your hexo folder, run <code>npm install hexo-asset-image --save</code> , and then make sure you have post_asset_folder: true in your _config.yml. Beware, <strong>_config.yml in your hexo folder, not the one in your theme folder</strong>. </p>
<p>After that, when you create a new post by hexo -n, a new folder with the same name as your post will be created. So the structure will be like <img src="/Jun-Blog/2016/05/24/insert-local-image-for-hexo-sites/structure.png" alt="this">. Got this! Then just use <code>![logo](MacGesture2-Publish/logo.jpg)</code> to insert logo.jpg to insert the figure.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is very common to insert images into a markdown file. Usually, we can easily make this by “![]()” and put the URL of the pictures into
    
    </summary>
    
    
      <category term="hexo" scheme="https://miaojunca.github.io/tags/hexo/"/>
    
      <category term="markdown" scheme="https://miaojunca.github.io/tags/markdown/"/>
    
      <category term="plugin" scheme="https://miaojunca.github.io/tags/plugin/"/>
    
  </entry>
  
  <entry>
    <title>Something about MapReduce(1)</title>
    <link href="https://miaojunca.github.io/2016/05/24/Something-about-MapReduce-1/"/>
    <id>https://miaojunca.github.io/2016/05/24/Something-about-MapReduce-1/</id>
    <published>2016-05-24T13:17:34.000Z</published>
    <updated>2016-05-24T15:07:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce is the foundation of Big data manipulation. The idea was first introduced from Google <a href="http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf" target="_blank" rel="external">paper</a>. One thing I like the best for google papers is that they never bullshit. You will never see wordy and deeply-mathematical presentations which cannot help you understand the basic idea at all. They want to make their work simple and understandable.<br><a id="more"></a></p>
<p>The idea is MapReduce is straightforward, too. The <strong>Map</strong> function processes a key/value pair to generate a set of intermediate key/value paris, and a <strong>reduce</strong> function that merges all intermediate values associated with the same intermediate key. I guess the two authors may do a lot of duplicated work on Grep or building an inverted index so that they want to convert it into a paralleled process. Although they are lots of platforms which inherit the MapReduce structure and make it much easier to implement such work, e.g., Spark/Hive/Pig. It is still necessary to understand how it works. </p>
<p>Basically, MapReduce fits problems which can be divide-and-conquer. After division, each part of the problem can be solved separately and in a similar way. For example, a common example is word-count. When you have 100M documents and you want to get the word count for each term, you can count words in each 100 documents and add them together. For each 100 documents, you will get pairs like <term, termcount="">. The terms are natural keys for MapReduce while they are unique and the objects we work on. After the Map process, we will have lots of term-count pairs. Values for the same key term will be summed up and reduce the pairs. Finally, we will have a clean term-count pair list and no duplicated keys in it.</term,></p>
<p>So the common steps a MapReduce process should be:</p>
<ol>
<li>Consider whether the problem can be divided into pieces and dealt with in the same way. If yes, whether the results of each piece can be combined to get the final outputs.</li>
<li>Determine the key/value pairs for each piece. Of course, have to know the input source first and consider how to convert it into key/value pairs as well.</li>
<li>The intermediate outputs from Mapper will be the inputs of the Reducer. Collect the values for the same key, and think about how to use them to get the final results.</li>
</ol>
<p>Looks simple. Right? There are some other mechanisms to ensure the MapReduce model to be fault-tolerant, load-balance… But so far these are not my concerns. There are some examples in the google paper. </p>
<ul>
<li>Word Count. </li>
</ul>
<p><img src="/Jun-Blog/2016/05/24/Something-about-MapReduce-1/wordcount.png" alt=""></p>
<ul>
<li>Distributed Grep. Mapper will emit a line if it matches a supplied pattern. Reducer just copy the supplied intermediate data to the output. So what are the keys? Seems we can use doc_id+line_num</li>
<li>Count of URL access Frequency. Just like word count, replace words to URLs.</li>
<li>Reverse Web-Link Graph. Mapper outputs <target, source="">. Targets are the links in the Source page. Reduces will concatenate the list of all source URL associated with a given target URL and emits <targes, list(source)="">. </targes,></target,></li>
<li>Term-Vector per Host. A term vector summarizes the<br>most important words that occur in a document or a set<br>of documents as a list of <word, frequency=""> pairs. The<br>map function emits a <hostname, term=""> vector<br>pair for each input document (where the hostname is<br>extracted from the URL of the document). The reduce<br>function is passed all per-document term vectors<br>for a given host. It adds these term vectors together,<br>throwing away infrequent terms, and then emits a final<br><hostname, term="" vector=""> pair.</hostname,></hostname,></word,></li>
<li>Inverted Index. Each documents will be put into Mapper. The Mapper will emit <term, docid=""> pairs, and the Reduces will concatenate all the docids and make a list. The outputs will be <term, list(docids)="">. If we make docid and term position together, we can also store this information.</term,></term,></li>
<li>Distributed sort. This is a little bit tricky to determine the key/value. We can’t just implement it like a merge sort while merge sort only has sub lists. So the strategy is Hash each line of the text to a particular reducer to make sure that $K_1$ &lt; $K_2$ =&gt; hash($K_1$) =&gt; hash($K_2$). If the keys are the prefixes of each line, then make sure the intermediate values are sent to reducer based on their prefix orders.</li>
</ul>
<p>More examples can be found <a href="https://courses.cs.washington.edu/courses/cse490h/08au/lectures/algorithms.pdf" target="_blank" rel="external">Here</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapReduce is the foundation of Big data manipulation. The idea was first introduced from Google &lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf&quot;&gt;paper&lt;/a&gt;. One thing I like the best for google papers is that they never bullshit. You will never see wordy and deeply-mathematical presentations which cannot help you understand the basic idea at all. They want to make their work simple and understandable.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="https://miaojunca.github.io/categories/Big-Data/"/>
    
    
      <category term="MapReduce" scheme="https://miaojunca.github.io/tags/MapReduce/"/>
    
      <category term="Big data" scheme="https://miaojunca.github.io/tags/Big-data/"/>
    
  </entry>
  
  <entry>
    <title>Solve a machine learning problem step by step</title>
    <link href="https://miaojunca.github.io/2016/04/26/Solve-a-machine-learning-problem-step-by-step/"/>
    <id>https://miaojunca.github.io/2016/04/26/Solve-a-machine-learning-problem-step-by-step/</id>
    <published>2016-04-26T18:32:11.000Z</published>
    <updated>2016-04-26T18:45:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Main frame for data science problems</strong></p>
<p>My main problem is lack of experiences in dealing such problems, e.g. Cases on Kaggle. To be honest, with the help of open source packages, it is easy to get a nice result. Fetching the python code from a top-k author and modify it is an easy way. However, in order to find a data scientist job, it is better to learn how to solve these problems from the very beginning.<br><a id="more"></a></p>
<p>Basically, we can divide the process into several steps: </p>
<ol>
<li>Collect data. On Kaggle, data are ready in .csv files, so it is easy to read them through Pandas. For other scenarios, we need to collect data by ourselves and store them. The techniques or tools we can use are Nutch, Scrapy, Mysql, Solr/Lucene. </li>
<li><p>Load Data. Pandas has a series of readers to read data in different formats, which is very nice. Check the list below for what you need: </p>
<ul>
<li>read_csv </li>
<li>read_excel </li>
<li>read_hdf </li>
<li>read_sql </li>
<li>read_json </li>
<li>read_msgpack (experimental) </li>
<li>read_html </li>
<li>read_gbq (experimental) </li>
<li>read_stata - read_sas </li>
<li>read_clipboard </li>
<li>read_pickle </li>
</ul>
</li>
<li><p>Data conversion. Usually, we use float vectors to represent each item and for the learning process, but the world is not so nice to provide such data only. So we may have free texts and categories. Luckily, we have Pandas and Sklearn to deal with them. </p>
<ul>
<li><p>Texts. Vectorizer. For exmaple, a commonly used Vectorizer from sklearn package is TfidfVectorizer: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">new_docs = [<span class="string">'He watches basketball and baseball'</span>, <span class="string">'Julie likes to play basketball'</span>, <span class="string">'Jane loves to play baseball'</span>] </div><div class="line">new_term_freq_matrix = tfidf_vectorizer.transform(new_docs) </div><div class="line"><span class="keyword">print</span> tfidf_vectorizer.vocabulary_ </div><div class="line"><span class="keyword">print</span> new_term_freq_matrix.todense()</div><div class="line"></div><div class="line"><span class="comment">#Terms are transformed to features, </span></div><div class="line">the value number <span class="keyword">is</span> the feature index of</div><div class="line">this term &#123;<span class="string">u'me'</span>: <span class="number">8</span>, <span class="string">u'basketball'</span>: <span class="number">1</span>, <span class="string">u'julie'</span>: <span class="number">4</span>, <span class="string">u'baseball'</span>: <span class="number">0</span>, </div><div class="line"><span class="string">u'likes'</span>: <span class="number">5</span>, <span class="string">u'loves'</span>: <span class="number">7</span>, <span class="string">u'jane'</span>: <span class="number">3</span>, <span class="string">u'linda'</span>: <span class="number">6</span>, <span class="string">u'more'</span>: <span class="number">9</span>, </div><div class="line"><span class="string">u'than'</span>: <span class="number">10</span>, <span class="string">u'he'</span>: <span class="number">2</span>&#125; </div><div class="line"></div><div class="line"><span class="comment">#tfidf vector for each document </span></div><div class="line">[[ <span class="number">0.57735027</span> <span class="number">0.57735027</span> <span class="number">0.57735027</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ] [ <span class="number">0.</span> <span class="number">0.68091856</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.51785612</span> <span class="number">0.51785612</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ] [ <span class="number">0.62276601</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.62276601</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.4736296</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ]]</div></pre></td></tr></table></figure>
</li>
<li><p>To convert categories into integers, we can use Pandas with a map function: </p>
</li>
</ul>
</li>
</ol>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Data are from the Titanic competition on Kaggle.com</span></div><div class="line"></div><div class="line">df[<span class="string">'Gender'</span>] = <span class="number">4</span> <span class="comment">#Genders are converted to integers by the map function with a dictionary </span></div><div class="line">df[<span class="string">'Gender'</span>] = df[<span class="string">'Sex'</span>].map( &#123;<span class="string">'female'</span>: <span class="number">0</span>, <span class="string">'male'</span>: <span class="number">1</span>&#125; ).astype(int)</div></pre></td></tr></table></figure>
</code></pre><ol>
<li><p>Data munging and data clean. </p>
<ul>
<li>If we want to remove a feature, just use Pandas’s drop() function. Droping items with NA features? Easy <code>pd.dropna()</code> </li>
<li>Simply filling NA features. <code>pd.fillna()</code>. Of course, there are lots of parameters we can use for filling. </li>
<li>Complex filling. Sometimes, we want to fill the missing values more reasonably. For example, use the combination of other different features to determine the lost feature values. </li>
<li>Feature creation. If we think we can make new features based on the current ones, it sometimes makes sense. For example, in the Titanic competition, we have the numbers of siblings and children. We can combine them to be a new features “familySize”. If we have the length and width of a rectangle, we can create a feature named “Area”. Actually, according to the <a href="http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf" target="_blank" rel="external">paper</a> about how to select features, features which have high variance with each other can also be complements. So don’t worry about the information overlap among features. </li>
<li>Normalization. It is necessary to normalize features into the same scale like 0 to 1. No matter we plan to use linear models or not, this can help get a more reasonable result. </li>
<li>Dimension deduction. PCA or LDA can reduce less important features and improve the learning speed. However, they have their own limit for particular scenarios. </li>
</ul>
</li>
<li><p>Training. Select appropriate models for training. If you don’t know the best choice, try linear models or GBDT/RandomForest. Usually, they are not bad. </p>
</li>
<li>Testing. No matter cross-validation or others. Don’t use the same data for both training and testing. </li>
<li>Use it. In Kaggle cases, use your trained model on the test data and submit it.</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Main frame for data science problems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My main problem is lack of experiences in dealing such problems, e.g. Cases on Kaggle. To be honest, with the help of open source packages, it is easy to get a nice result. Fetching the python code from a top-k author and modify it is an easy way. However, in order to find a data scientist job, it is better to learn how to solve these problems from the very beginning.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="https://miaojunca.github.io/tags/machine-learning/"/>
    
      <category term="data mining" scheme="https://miaojunca.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Concepts (2)</title>
    <link href="https://miaojunca.github.io/2016/04/20/Machine-Learning-Concepts-2/"/>
    <id>https://miaojunca.github.io/2016/04/20/Machine-Learning-Concepts-2/</id>
    <published>2016-04-20T13:51:09.000Z</published>
    <updated>2016-04-20T17:27:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>Continue to the previous post.</p>
<h4 id="Bias-vs-Variance"><a href="#Bias-vs-Variance" class="headerlink" title="Bias vs Variance"></a>Bias vs Variance</h4><p>These two concepts have confused me for a long time, until I read something in <strong>Pattern Recognition and Machine Learning</strong>. Generally, bias is the difference between the real model $G(x)$ and the model we learn $H(x)$; variance is the difference between $H(x)$ and $E(H(x))$. Here is a good picture to explain them:<br><img src="http://blog.fliptop.com/wp-content/uploads/2015/01/Bias_Variance.jpg" alt=""><br><a id="more"></a></p>
<p>The red spot is our target, or we can say $G(x)$. High bias means the learned model $H(x)$ is quite different from the real model, so we will get bad performance with this model. A high variance means the expectation of $H(x)$ is close to $E(G(x))$, but the result $H(x_i)$ for each sample $x_i$ is not close to $G(x_i)$. Ideally, we want both of them to be low. However, that is a kind of trade-off. The decrease of one will cause an increase of the other one. High variance learning methods may be able to represent the training set very well. But the world is not perfect, it will try to match everything, including the noise. So the model we get will be very complex and may fail for more new data. On the contrary, high-bias models are usually very simple, but fail to capture the important regularities. Models with low bias are usually very complex and obtain low training error. This complexity makes it hard to be generalized for other new samples, which gives high variance. </p>
<h4 id="Overfit-vs-Underfit"><a href="#Overfit-vs-Underfit" class="headerlink" title="Overfit vs Underfit"></a>Overfit vs Underfit</h4><p>These two concepts are connect with the above two. High variance leads to overfit, and high bias causes underfit. The following figure can describe them well.<br><img src="http://blog.fliptop.com/wp-content/uploads/2015/03/highvariance-300x254.png" alt="">. When training error is smaller and smaller, the probability of overfit increases because the model will be very complex (As we say above, this will cause a high variance because the model is too accurate for the training data. It will match noisy samples as well). A high-bias model will get a high training error with a simple form. So it can’t fit the training set well enough, or we can say “underfit”.</p>
<p>The Figures are from <a href="http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/" target="_blank" rel="external">Fliptop Blog</a>. This post gives a better description about these concepts.</p>
<h4 id="Regression-VS-Classification"><a href="#Regression-VS-Classification" class="headerlink" title="Regression VS Classification"></a>Regression VS Classification</h4><p>Regression methods are used to fit data for continuous targets, e.g., predict stock market value. Classification methods are for discrete targets, e.g., predict whether tomorrow will be sunny or rainy. </p>
<h4 id="Reinforced-learning"><a href="#Reinforced-learning" class="headerlink" title="Reinforced learning"></a>Reinforced learning</h4><p>This is different from deep learning. It does not have multiple layers. Instead, it has a environment state set, an action set and a set of state transition rules. Unlike traditional machine learning methods, it cannot be trained in one time. A typical scenario is playing chess. Each decision is determined by previous one or more steps. So the learning process is step by step, too. In this case, we setup a reward function instead of an error function. If we think a step is good, then we will give the learning agent a positive reward. Otherwise, it will be given a punishment. The problem becomes finding a step path with the maximum reward. </p>
<p>Reinforced learning has been widely used in auto helicopter, robots, industrial control… It is a elegant solution for complex Markov Decision Process(MDP). A brief tutorial can be found <a href="http://www.cs.ubc.ca/~murphyk/Bayes/pomdp.html" target="_blank" rel="external">here</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Continue to the previous post.&lt;/p&gt;
&lt;h4 id=&quot;Bias-vs-Variance&quot;&gt;&lt;a href=&quot;#Bias-vs-Variance&quot; class=&quot;headerlink&quot; title=&quot;Bias vs Variance&quot;&gt;&lt;/a&gt;Bias vs Variance&lt;/h4&gt;&lt;p&gt;These two concepts have confused me for a long time, until I read something in &lt;strong&gt;Pattern Recognition and Machine Learning&lt;/strong&gt;. Generally, bias is the difference between the real model $G(x)$ and the model we learn $H(x)$; variance is the difference between $H(x)$ and $E(H(x))$. Here is a good picture to explain them:&lt;br&gt;&lt;img src=&quot;http://blog.fliptop.com/wp-content/uploads/2015/01/Bias_Variance.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://miaojunca.github.io/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="https://miaojunca.github.io/tags/machine-learning/"/>
    
      <category term="data mining" scheme="https://miaojunca.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Concepts(1)</title>
    <link href="https://miaojunca.github.io/2016/04/18/Machine-Learning-Concepts/"/>
    <id>https://miaojunca.github.io/2016/04/18/Machine-Learning-Concepts/</id>
    <published>2016-04-18T04:05:06.000Z</published>
    <updated>2016-04-18T14:59:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>There are lots of machine learning concepts which look complex and mysterious. Before learning the ML techniques, it is necessary to understand them clearly. </p>
<h4 id="Machine-learning-vs-Data-mining"><a href="#Machine-learning-vs-Data-mining" class="headerlink" title="Machine learning vs Data mining"></a>Machine learning vs Data mining</h4><p>These two are very confusing for me. Honestly, I don’t think they have essential differences. Both of them means using computer techniques and statistic models to learn latent patterns from data. From the <a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/cccf07.pdf" target="_blank" rel="external">review</a>, Prof. Zhou says that “Machine learning” came from the AI area. Its purpose is making machines learn like real persons. The concept “Data mining” first came from the database area. It is close to “Knowledge discovery”. It is used to discovery useful knowledge from big amount of data. So we can consider DM as the inter-discipline of ML and database. A more formal definition from <a href="https://class.coursera.org/ntumlone-003/lecture/11" target="_blank" rel="external">coursera</a> by Prof.Hsuan-Tien Lin is </p>
<ul>
<li>Machine learning: use data to computer hypothesis g that approximate target f</li>
<li>Data mining: use (huge) data to find property.</li>
<li>Traditional ML techniques concern on small scale data and DM deals with big data. ML is suitable for pattern recognition and prediction, DM can discovery associations among features(e.g., beer and diaper). <a id="more"></a>
</li>
</ul>
<h4 id="Supervised-learning-vs-Unsupervised-learning"><a href="#Supervised-learning-vs-Unsupervised-learning" class="headerlink" title="Supervised learning vs Unsupervised learning"></a>Supervised learning vs Unsupervised learning</h4><p>Quite clear. Supervised learning rely on labelled data for training. It has very clear outputs. For example, classification or regression are this kind of applications. On the contrary, unsupervised learning do not need labelled data, so we cannot know that exact results we will get. A typical example is clustering. We know the data can be categorized into several groups, but we don’t know the group amount. In that case, there is no best result in general. Of course, to apply unsupervised learning, researchers proposed some metrics to evaluate its performance. </p>
<h4 id="Deep-learning-vs-Shallow-learning"><a href="#Deep-learning-vs-Shallow-learning" class="headerlink" title="Deep learning vs Shallow learning"></a>Deep learning vs Shallow learning</h4><p>Deep learning is extremely hot recently. AlphaGo is based on the technique. Facebook’s face recognition is based on it. It seems to be the god for AI. Is that true? Why is it “deep”?</p>
<p>Traditional machine learning methods are called “shallow learning” because they only have no more than 1 hidden layer. What does that mean? These methods will not construct very complex non-linear model. For example, logistic regression is used to find a formula like $\frac{1}{1 + e^{-\theta X}}$. $\theta X$ is actually a dot product of $\theta$ and $X$, or we can say this function is essentially linear. SVM, no matter what kernel it uses, update the same formula by different data to obtain a better performance. Generally, the formula is already determined. What we do is optimize the parameters.</p>
<p>One may ask about why traditional neural network is shallow too while it can be multi-layer. Let me talk about my opinion about machine learning first. Linear models are the simplest models. They can used to fit data which are not complex. That is why all ML courses start from them. In real world, however, things are not so perfect. We can say linear models are not suitable for most cases. Then how can we model non-linear data? A straightforward way is build a linear model over multiple linear sub-models. For example as follows:<br><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/450px-Colored_neural_network.svg.png" alt="ANN"> </p>
<p>Each node in the hidden layer is the result of a linear model whose features are the inputs. Each node in the output layer is the result of a linear model of the hidden layer nodes. If we have more layers, we can build a very complex model and ideally fit any curves. Yes, this is the idea of neural networks. </p>
<p>Wait, since we can have a multi-layer NN, why is it shallow? The problem is, traditional NN is optimized by the classic Back propagation(BP) method. A very detailed introduction is <a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="external">here</a>. I will not introduce how it works here. Because BP optimizes parameters layer by layer, the residues used for mid-layer optimization will be very small if there are many layers. So practically, there will not be more than 2 layers. </p>
<p>How about deep learning? The concept “deep learning” is actually based on NN, and it is a multi-layer NN. The main difference is Professor Hinton found a new way to train the model and avoid the problem of BP. It can therefore support more layers. Facebook builds a 9-layer NN to recognize people’s face and obtain a 97.25% precision. I will read more references about deep learning and write a post in the future.  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;There are lots of machine learning concepts which look complex and mysterious. Before learning the ML techniques, it is necessary to understand them clearly. &lt;/p&gt;
&lt;h4 id=&quot;Machine-learning-vs-Data-mining&quot;&gt;&lt;a href=&quot;#Machine-learning-vs-Data-mining&quot; class=&quot;headerlink&quot; title=&quot;Machine learning vs Data mining&quot;&gt;&lt;/a&gt;Machine learning vs Data mining&lt;/h4&gt;&lt;p&gt;These two are very confusing for me. Honestly, I don’t think they have essential differences. Both of them means using computer techniques and statistic models to learn latent patterns from data. From the &lt;a href=&quot;http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/cccf07.pdf&quot;&gt;review&lt;/a&gt;, Prof. Zhou says that “Machine learning” came from the AI area. Its purpose is making machines learn like real persons. The concept “Data mining” first came from the database area. It is close to “Knowledge discovery”. It is used to discovery useful knowledge from big amount of data. So we can consider DM as the inter-discipline of ML and database. A more formal definition from &lt;a href=&quot;https://class.coursera.org/ntumlone-003/lecture/11&quot;&gt;coursera&lt;/a&gt; by Prof.Hsuan-Tien Lin is &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning: use data to computer hypothesis g that approximate target f&lt;/li&gt;
&lt;li&gt;Data mining: use (huge) data to find property.&lt;/li&gt;
&lt;li&gt;Traditional ML techniques concern on small scale data and DM deals with big data. ML is suitable for pattern recognition and prediction, DM can discovery associations among features(e.g., beer and diaper).&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://miaojunca.github.io/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="https://miaojunca.github.io/tags/machine-learning/"/>
    
      <category term="data mining" scheme="https://miaojunca.github.io/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>Build a light blog on Github by Hexo</title>
    <link href="https://miaojunca.github.io/2016/04/15/Build-a-light-blog-on-Github-by-Hexo/"/>
    <id>https://miaojunca.github.io/2016/04/15/Build-a-light-blog-on-Github-by-Hexo/</id>
    <published>2016-04-15T14:55:52.000Z</published>
    <updated>2016-10-16T00:47:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>In order to have my own blog, I have tried a lot of ways. A popular one is buy a domain name and a space,  download wordpress, use a template with plenty of plugins. The problem is, I have to select themes and plugins to satisfied my needs, e.g., code highlight or Markdown support. Is that too hard? Not really. But that will definitely cost much time, especially on dealing with css problems and plugin conflicts. Since usually I want to write some about what I learned in my area, it is better to start the blogging as easy as possible.<br><a id="more"></a><br>Github gives me the chance. It is the first time I know it can support blogs in a handy way. There are lots of posts talking about how to build it, and the guidance from Github is good enough. Here I want to record how I use <a href="https://hexo.io/" target="_blank" rel="external">hexo</a> to get this done and what problems I have met.</p>
<p>Hexo is a jekyll like tool for generating static website, which is quite suitable for Github pages. I have tried jekyll. It is good, but still not so convenient as hexo. I have to update everything as using a normal git repository with commit/push operations. On the contrary, hexo can get these done with one single command.</p>
<p>Installing hexo is very easy. First of all, make sure node.js is installed. On mac, just use </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install node</div></pre></td></tr></table></figure>
<p>Of course, you have to install git as well. Then what’s next? Another two commands are enough</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-cli -g</div><div class="line">npm install hexo-deployer-git --save</div></pre></td></tr></table></figure>
<p>Then everything is done.</p>
<p>Now create a new folder for your blog site. For example, ~/hexo as I did. Following the next steps:</p>
<ol>
<li>enter the hexo folder and run <code>hexo init</code></li>
<li>config _config.yml for your own needs</li>
<li>execute `hexo generate’ to generate a template site</li>
<li>run <code>hexo server</code> and then open <code>localhost:4000</code> in your browser</li>
<li>if you can see the website, then everything is fine</li>
<li>use <code>hexo new [layout] &quot;title&quot;</code> to create a new post. For example, <code>hexo new post &quot;hello world&quot;</code>. Then you will have a new <strong>md</strong> file in source/_posts folder.</li>
<li>Use a editor to write whatever you like and save it. Here I highly recommend a markdown editor <a href="http://macdown.uranusjr.com/" target="_blank" rel="external">Macdown</a>. It is open-source and has enough functions for most blogger. </li>
<li><code>hexo deploy</code> to upload your post to your github blog.</li>
</ol>
<p>There are several things we need to concern. The most important thing is the _config.yml. This file controls all the settings of your blog. It is easy to setup title/description or other site attributes. For the URL part, take care. Initially, I think I don’t have a personal web space and domain name, so I needn’t change this. The result is, no matter how I deploy it, my blog on github does not execute the css style file. Also, every link on the site will be directed to <code>404</code> page. So even you just use your github url, define it in the _config.yml file as follows please</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">url: http://james97.github.io/   #Not HTTPS!!</div><div class="line">root: /Jun-blog/ #project name</div></pre></td></tr></table></figure>
<p>Another point is about the deploy settings. Here is mine</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy: </div><div class="line">    type: git</div><div class="line">    repository: git@github.com:james97/Jun-blog.git</div><div class="line">    branch: gh-pages</div></pre></td></tr></table></figure>
<p>Some old posts set type to <code>github</code>. It is not correct now. The branch issue is also important. The real blog shown to readers is always in your <code>gh-pages</code> branch. When you create a repository for the blog, you will have a <code>master</code> branch at first. Then go to settings and click <code>Launch automatic page generator</code>, you will have a <code>gh-pages</code> branch automatically. Every time you want to update your blog, modify this branch. The <code>master</code> branch is used for your real projects, not the blogs.</p>
<p>If you want to use some fancy themes, you can find them <a href="https://hexo.io/themes/" target="_blank" rel="external">here</a>. All of them are on Github.com. Clone them into the <code>/themes/</code> folder and update them as normal git projects. Use <code>hexo generate</code> to update the theme. After the update, check how it works by run <code>hexo server</code> locally. A common problem is <code>cannot find index.htm</code>. The reason is hexo cannot find the according theme set in _config.yml. Remember to keep a blank between <code>theme:</code> and the theme name. Another problem is when your posts/pages are not compatible with the theme. In this case, there will a lot of error when executing <code>hexo g</code>. Currently I am using the <a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="external">yilia</a> theme, which is pretty cool. </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In order to have my own blog, I have tried a lot of ways. A popular one is buy a domain name and a space,  download wordpress, use a template with plenty of plugins. The problem is, I have to select themes and plugins to satisfied my needs, e.g., code highlight or Markdown support. Is that too hard? Not really. But that will definitely cost much time, especially on dealing with css problems and plugin conflicts. Since usually I want to write some about what I learned in my area, it is better to start the blogging as easy as possible.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://miaojunca.github.io/categories/Programming/"/>
    
    
      <category term="hexo" scheme="https://miaojunca.github.io/tags/hexo/"/>
    
      <category term="github" scheme="https://miaojunca.github.io/tags/github/"/>
    
      <category term="blog" scheme="https://miaojunca.github.io/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://miaojunca.github.io/2016/04/13/hello-world/"/>
    <id>https://miaojunca.github.io/2016/04/13/hello-world/</id>
    <published>2016-04-14T02:44:14.000Z</published>
    <updated>2016-04-15T16:00:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
